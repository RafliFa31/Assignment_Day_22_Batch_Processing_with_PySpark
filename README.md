# Assignment_Day_22_Batch_Processing_with_PySpark
# Batch Processing ETL with PySpark & Apache Airflow
@ğŸ“Œ Project Overview
ETL pipeline yang dibangun menggunakan Apache Airflow dan PySpark untuk menganalisis data penerbangan pelanggan, kemudian menyimpan hasilnya ke dalam PostgreSQL dan file CSV.

@ğŸ§° Tools & Technologies
Apache Airflow â€“ untuk menjadwalkan dan mengorkestrasi job ETL

PySpark â€“ untuk proses transformasi data dalam skala besar

PostgreSQL â€“ penyimpanan hasil akhir (output table)

Docker Compose â€“ untuk menjalankan seluruh stack dalam container

@ğŸ”„ Pipeline Overview
Extract
Membaca file CSV hasil analisis sebelumnya yang telah dibersihkan.

Transform
Menganalisis pelanggan berdasarkan tingkat loyalitas dan frekuensi terbang.

Load
Menyimpan hasil analisis ke:

Tabel PostgreSQL: frequent_flyers_by_loyalty

File CSV: output/frequent_flyers_by_loyalty.csv

@ğŸ“‚ Struktur File
kotlin
Copy
Edit
BatchPro22/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ batch_etl_dag.py
â”œâ”€â”€ pyspark_scripts/
â”‚   â””â”€â”€ batch_etl_day.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ customer_flight_activity.csv
â”‚   â”œâ”€â”€ customer_loyalty_history.csv
â”‚   â””â”€â”€ output/
â”œâ”€â”€ jars/
â”‚   â””â”€â”€ postgresql-42.2.5.jar
â”œâ”€â”€ docker-compose.yaml
â””â”€â”€ Dockerfile.airflow
#ğŸ—“ï¸ Airflow DAG
Nama DAG: batch_etl_pyspark

Lokasi File: dags/batch_etl_dag.py

Deskripsi: Menjalankan skrip PySpark batch_etl_day.py via spark-submit

#ğŸš€ Menjalankan Proyek
bash
Copy
Edit
docker-compose up --build
Setelah itu, akses antarmuka berikut:

Airflow UI: http://localhost:8088

Spark UI: http://localhost:8080

#ğŸ“¦ Output
Hasil ETL akan disimpan ke:

PostgreSQL Table: frequent_flyers_by_loyalty

CSV File: data/output/frequent_flyers_by_loyalty.csv

