# Assignment_Day_22_Batch_Processing_with_PySpark
# Batch Processing ETL with PySpark & Apache Airflow
#ğŸ“Œ Project Overview

ETL pipeline yang dibangun menggunakan Apache Airflow dan PySpark untuk menganalisis data penerbangan pelanggan, kemudian menyimpan hasilnya ke dalam PostgreSQL dan file CSV.

#ğŸ§° Tools & Technologies

Apache Airflow â€“ untuk menjadwalkan dan mengorkestrasi job ETL

PySpark â€“ untuk proses transformasi data dalam skala besar

PostgreSQL â€“ penyimpanan hasil akhir (output table)

Docker Compose â€“ untuk menjalankan seluruh stack dalam container

#ğŸ”„ Pipeline Overview

-Extract

Membaca file CSV hasil analisis sebelumnya yang telah dibersihkan.

-Transform

Menganalisis pelanggan berdasarkan tingkat loyalitas dan frekuensi terbang.

-Load

Menyimpan hasil analisis ke:

Tabel PostgreSQL: frequent_flyers_by_loyalty

File CSV: output/frequent_flyers_by_loyalty.csv


#ğŸ—“ï¸ Airflow DAG

nama DAG: batch_etl_pyspark

Lokasi File: dags/batch_etl_dag.py

Deskripsi: Menjalankan skrip PySpark batch_etl_day.py via spark-submit

#ğŸš€ Menjalankan Proyek
bash
Copy
Edit
docker-compose up --build
Setelah itu, akses antarmuka berikut:
Airflow UI: http://localhost:8088
Spark UI: http://localhost:8080

#ğŸ“¦ Output

Hasil ETL akan disimpan ke:

PostgreSQL Table: frequent_flyers_by_loyalty

CSV File: data/output/frequent_flyers_by_loyalty.csv

